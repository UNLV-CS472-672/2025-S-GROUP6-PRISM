"""lint is dumb.

ooooooooo.   ooooooooo.   ooooo  .oooooo..o ooo        ooooo
`888   `Y88. `888   `Y88. `888' d8P'    `Y8 `88.       .888'
 888   .d88'  888   .d88'  888  Y88bo.       888b     d'888
 888ooo88P'   888ooo88P'   888   `"Y8888o.   8 Y88. .P  888
 888          888`88b.     888       `"Y88b  8  `888'   888
 888          888  `88b.   888  oo     .d8P  8    Y     888
o888o        o888o  o888o o888o 8""88888P'  o8o        o888o

Created by Jesse Ortega, Spring 2025, vers. 1.1
populate_prism_db.py

The purpose of this script is to populate the `prism_db` or `prism_db_dev` PostgreSQL
databases with fabricated data for testing and development of the PRISM platform.
"""

# Import necessary libraries:
# - pandas: for reading, transforming, and writing tabular data
# - sqlalchemy: for database connection and SQL execution
# - decouple: for loading environment variables from a .env file
# - pathlib: for convenient and safe filesystem path handling
# - zipfile: for extracting compressed bulk submission archives
# - shutil: for file and directory operations (copying, deleting, etc.)
# - subprocess: for executing external commands like moss.pl
# - glob: for pattern-based file path matching
# - logging: for structured logging
# - typing: for return type annotations
# - time: for controlling time behavior (e.g., timezone conversion)
import pandas as pd
from sqlalchemy import create_engine, text
from decouple import config
from pathlib import Path
import zipfile
import shutil
import subprocess
import glob
import logging
from typing import Tuple
import time

"""
=======================================================================
PRISM Database Population Script
=======================================================================

This script depends on a set of pre-generated CSV and ZIP files to build
a complete dataset, including users, courses, assignments, submissions,
and similarity reports generated by MOSS.

-----------------------------------------------------------------------
/PRISM/data/final  (862 files total)
-----------------------------------------------------------------------
Directory containing bulk submission data for all fabricated assignments
across multiple semesters, courses, and sections.

Each assignment must include:
    - A `.csv` file containing CodeGrade metadata (student IDs and grades)
    - A `.zip` archive with the corresponding student submission files

Example pair:
    PRISM/data/final/CS 135 1001 - 2024 Fall - Assignment 0.csv
    PRISM/data/final/CS 135 1001 - 2024 Fall - Assignment 0.zip

This directory is critical — do not run the script without it.

-----------------------------------------------------------------------
"""
BASE_PATH = Path("/PRISM/data/database_dataset")
"""
-----------------------------------------------------------------------
The following CSV files must exist in the BASE_PATH directory:

1. assignments.csv
   Description: Metadata for all assignments in the dataset
   Columns: subject, catalog_number, term, year, session, assignment_number,
            due_date, lock_date, title, has_base_code, language, has_policy
   Row count: 46

2. course_catalog.csv
   Description: Master list of supported course offerings
   Columns: name, subject, catalog_number, course_title, course_level
   Row count: 4

3. course_instances.csv
   Description: Defines individual course sections, Canvas IDs, and term details
   Columns: section_number, canvas_course_id, course_name, year, term, session
   Row count: 20

4. semesters.csv
   Description: Metadata for academic terms and sessions
   Columns: name, year, term, session
   Row count: 18

5. students.csv
   Description: Synthetic student roster used for course enrollments
   and assignment submissions
   Columns: first_name, last_name, ace_id, email, nshe_id, codeGrade_id, canvas_course_id
   Row count: 755

6. users.csv
   Description: Fabricated user accounts (professors and TAs)
   Columns: password, last_login, is_superuser, username, is_staff,
            is_active, date_joined, email, first_name, last_name, is_admin
   Row count: 16

-----------------------------------------------------------------------
MOSS EXECUTABLE (Required for Similarity Analysis)
-----------------------------------------------------------------------
The script requires the `moss.pl` executable and its supporting files to be present at:

"""
MOSS_EXEC = "./moss.pl"
MOSS_WORKING_DIRECTORY = "/PRISM/mosslocal/"
"""

The executable is required for the generation of similarity reports on student submissions.
Without it, MOSS report generation and related database entries will be skipped or fail.
"""

# Initialize SQLAlchemy engine using database credentials loaded from the .env file
ENGINE = create_engine(f"postgresql+psycopg2://{config("DB_USER")}:{config("DB_PASSWORD")}@{config("DB_HOST")}:{config("DB_PORT")}/{config("DB_NAME")}")

# Path Constants
BASE_PATH_PRISM = Path("/PRISM/data/")

# Languages supported by MOSS. Referenced when converting strings retrieved from database
# to their MOSS counterpart
LANGUAGES = {
    "C": "c",
    "C++": "cc",
    "java": "java",
    "ml": "ml",
    "ocaml": "ocaml",
    "ruby": "ruby",
    "pascal": "pascal",
    "ada": "ada",
    "lisp": "lisp",
    "scheme": "scheme",
    "haskell": "haskell",
    "fortran": "fortran",
    "ascii": "ascii",
    "vhdl": "vhdl",
    "perl": "perl",
    "matlab": "matlab",
    "python": "python",
    "mips": "mips",
    "prolog": "prolog",
    "spice": "spice",
    "vb": "vb",
    "csharp": "csharp",
    "modula2": "modula2",
    "a8086": "a8086",
    "javascript": "javascript",
    "plsql": "plsql",
    "verilog": "verilog",
    "tcl": "tcl",
    "hc12": "hc12",
    "asm": "asm"
}

# Database tables within prism_db/prism_db_dev
# This script populates the uncommented tables below
# NOTE:
# - "assignments_constraints" and "assignments_policyviolations" are currently not populated
#   due to the source code be written in non-english language.
# - "assignments_basefiles" is left unpopulated because the plagiarism dataset does not
#   include any base files.

DB_TABLES = [
    "assignments_assignments",
    # "assignments_basefiles",
    "assignments_bulksubmissions",
    # "assignments_constraints",
    # "assignments_policyviolations",
    "assignments_requiredsubmissionfiles",
    "assignments_submissions",

    # "auth_group",
    # "auth_group_permissions",
    # "auth_permission",

    # "authtoken_token",

    # "cheating_cheatinggroupmembers",
    # "cheating_cheatinggroups",
    # "cheating_confirmedcheaters",
    # "cheating_flaggedstudents",
    # "cheating_longitudinalcheatinggroupinstances",
    # "cheating_longitudinalcheatinggroupmembers",
    # "cheating_longitudinalcheatinggroups",
    "cheating_submissionsimilaritypairs",

    "courses_coursecatalog",
    "courses_courseinstances",
    "courses_semester",
    "courses_professorenrollments",
    "courses_professors",
    "courses_studentenrollments",
    "courses_students",
    "courses_teachingassistantenrollments",
    "courses_teachingassistants",

    # "django_admin_log",
    # "django_content_type",
    # "django_migrations",
    # "django_session",

    "users_user",
    # "users_user_groups",
    # "users_user_user_permissions",
]

# Basic configuration for logging
logging.basicConfig(
    level=logging.INFO,
    format="{asctime} - {levelname} - {message}",
    style="{",
    datefmt="%Y-%m-%d %H:%M",
)

logging.Formatter.converter = time.localtime


def main() -> None:
    """
    Run the database population script.

    Configures Pandas display settings for debugging and visualization,
    then initiates the database build process.

    Parameters
    ----------
    None

    Returns
    -------
    None
    """
    # Show all columns when printing DataFrames
    pd.set_option("display.max_columns", None)

    # (Optional) Show all rows — currently commented out for readability
    # pd.set_option("display.max_rows", None)

    # Do not truncate long column values when printed
    pd.set_option('display.max_colwidth', None)

    # Begin database population. Set to True to clear existing data first.
    build_database(True)


def build_database(FF: bool = False) -> None:
    """
    build_database: Driver function that populates the PRISM database with synthetic data for testing or development.

    Parameters
    ----------
    FF : bool, optional
        If True, clears all existing assignment and course-related data before repopulating.

    Returns
    -------
    None
    """
    assignments_dir = BASE_PATH_PRISM / "assignments"

    logging.info("Starting database population...")

    try:
        if FF:
            logging.warning("Friendly fire enabled. Removing existing data and resetting tables.")
            if assignments_dir.exists() and assignments_dir.is_dir():
                shutil.rmtree(assignments_dir)
            friendly_fire()

        logging.info("Populating 'users_user' table...")
        populate_users_user()
        # read_users_user()

        logging.info("Populating 'courses_professors' and 'courses_teachingassistants'...")
        populate_courses_professors_and_TAs()
        # read_professors_and_TAs()

        logging.info("Populating 'courses_coursecatalog'...")
        populate_course_catalog()
        # read_course_catalog()

        logging.info("Populating 'courses_semester'...")
        populate_courses_semester()
        # read_courses_semester()

        logging.info("Populating 'courses_courseinstances'...")
        populate_course_instances()
        # read_course_instances()

        logging.info("Populating 'courses_professorenrollments' and 'courses_teachingassistantenrollments'...")
        populate_professor_and_TAs_enrollment()
        # read_professors_and_TAs_enrollment()

        logging.info("Populating 'courses_students' or 'courses_studentenrollments' tables....")
        populate_course_student_and_enrollments()
        # read_course_student()

        logging.info("Populating 'assignments_assignments' table...")
        populate_assignments()
        # read_assignments()

        logging.info("Building filesystem structure, then populating 'assignments_bulksubmissions' and 'courses_courseassignmentcollaboration'...")
        populate_filesystem_and_bulk_submissions(write_files=True)
        # read_bulksubmissions()
        # read_assignmentcollaborations()

        logging.info("Populating 'assignments_requiredsubmissionfiles' table...")
        populate_requiredsubmissionfiles()
        # read_requiredsubmissionfiles()
        # read_basefiles()

        logging.info("Running MOSS similarity reports...")
        generate_moss_reports()

        logging.info("Populating 'assignments_submissions' table...")
        populate_submissions()

        logging.info("Parsing MOSS reports and populating 'cheating_submissionsimilaritypairs' table...")
        parse_moss_populate_similarities()

        logging.info("Database population completed successfully.")

    except Exception:
        logging.exception("Database population failed due to an error.")
        raise


def flush_rows(table_name: str) -> None:
    """
    flush_rows: Delete all rows from a given table and reset its primary key sequence.

    This function uses the TRUNCATE command with RESTART IDENTITY to remove all data
    from the specified table and reset its auto-incrementing primary key back to 1.
    CASCADE ensures that any dependent foreign key records in related tables are also removed.

    Parameters
    ----------
    table_name : str
        The name of the table to be cleared.

    Returns
    -------
    None
    """
    try:
        with ENGINE.connect() as conn:
            # Truncate table and reset primary key sequence (CASCADE handles FK constraints)
            conn.execute(text(f"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE;"))
            conn.commit()
    except Exception:
        logging.exception("An error occurred during flush_rows().")
        raise


def friendly_fire() -> None:
    """
    friendly_fire: Clears all fabricated data from key database tables to reset the state of the system.

    This function is typically used before re-populating the database during testing
    or development. It truncates all relevant tables and resets their primary key
    sequences to ensure consistent auto-increment values.

    Parameters
    ----------
    None

    Returns
    -------
    None
    """
    try:
        with ENGINE.connect() as conn:
            for table_name in DB_TABLES:
                conn.execute(text(f"TRUNCATE TABLE {table_name} RESTART IDENTITY CASCADE;"))
                conn.commit()
        logging.info("Successfully truncated and reset populated tables.")

    except Exception:
        logging.exception("An error occurred during friendly_fire(). Database may be partially cleared.")
        raise


def populate_users_user() -> None:
    """
    populate_users_user: Populates the 'users_user' table in the database with fabricated user data.

    Loads user data from 'users_user.csv' located at BASE_PATH, converts the data
    into appropriate types, and inserts the rows into the database. This table
    includes professors and teaching assistants, along with admin metadata.

    Parameters
    ----------
    None

    Returns
    -------
    None

    Affected Table
    --------------
    - users_user
    """
    try:
        # Column names and types
        boolean_columns = ["is_superuser", "is_staff", "is_active", "is_admin"]
        date_columns = ["last_login", "date_joined"]
        string_columns = ["password", "username", "email", "first_name", "last_name"]

        # Load csv data into dataframe
        df_users = pd.read_csv(BASE_PATH / "users.csv")

        # Alter column data types
        for col in date_columns:
            df_users[col] = pd.to_datetime(df_users[col])
        df_users[boolean_columns] = df_users[boolean_columns].astype(bool)
        df_users[string_columns] = df_users[string_columns].astype(str)

        # Append dataframe's contents to database table
        df_users.to_sql("users_user", ENGINE, if_exists="append", index=False)

        logging.info("Successfully populated 'users_user' table.")

    except Exception:
        logging.exception("Failed to populate 'users_user' table.")
        raise


def read_users_user(print_df: bool = False) -> pd.DataFrame:
    """
    read_users_user: Reads and returns the contents of the 'users_user' table as a Pandas DataFrame.

    This function queries all rows from the 'users_user' table, converts the relevant
    columns to appropriate types, and optionally prints the DataFrame.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the DataFrame and its data types to the console. Default is False.

    Returns
    -------
    df_users : pd.DataFrame
        A DataFrame containing user records from the database.

        Columns:
        - id ('int64'): Primary key
        - password (str): Hashed user password
        - last_login (pd.datetime): Timestamp of last login
        - is_superuser (bool): Whether the user has superuser privileges
        - username (str): Unique login name
        - is_staff (bool): Whether the user is staff
        - is_active (bool): Whether the account is active
        - date_joined (pd.datetime): When the account was created
        - email (str): Email address
        - first_name (str): First name
        - last_name (str): Last name
        - is_admin (bool): Whether the user has admin-level privileges
    """
    try:
        # Column names and types
        bigint_columns = ["id"]
        boolean_columns = ["is_superuser", "is_staff", "is_active", "is_admin"]
        date_columns = ["last_login", "date_joined"]
        string_columns = ["password", "username", "email", "first_name", "last_name"]

        # Load users_user table into a dataframe
        df_users = pd.read_sql("SELECT * FROM users_user", ENGINE)

        # Alter column data types
        df_users[bigint_columns] = df_users[bigint_columns].astype("int64")
        df_users[boolean_columns] = df_users[boolean_columns].astype(bool)
        for col in date_columns:
            df_users[col] = pd.to_datetime(df_users[col], utc=True)
        df_users[string_columns] = df_users[string_columns].astype(str)

        # Print dataframe
        if print_df:
            print(f"df_users:\n{df_users.dtypes}\n{df_users}")

        return df_users

    except Exception:
        logging.exception("Failed to read 'users_user'")
        raise


def populate_courses_professors_and_TAs() -> None:
    """
    populate_courses_professors_and_TAs: Populates the 'courses_professors' and 'courses_teachingassistants' tables.

    This function loads user IDs from the 'users_user' table and evenly splits them
    into two groups: one for professors and one for teaching assistants. These groups
    are then written to their respective course role tables.

    Assumes that the total number of users is divisible by 2 and that all users are
    either professors or teaching assistants.

    Parameters
    ----------
    None

    Returns
    -------
    None

    Affected Table
    --------------
    courses_professors
    courses_teachingassistants
    """
    try:
        # Read table, project columns, and rename columns
        df_users = read_users_user()[["id"]].rename(columns={"id": "user_id"})

        # Evenly split df_users to df_professors and df_TAs
        midpoint = len(df_users) // 2
        df_professors = df_users.iloc[:midpoint]
        df_TAs = df_users.iloc[midpoint:]

        # Append each dataframe's contents to their respective tables in the database
        df_professors.to_sql("courses_professors", ENGINE, if_exists="append", index=False)
        df_TAs.to_sql("courses_teachingassistants", ENGINE, if_exists="append", index=False)

        logging.info("Successfully populated 'courses_professors' and 'courses_teachingassistants' tables.")

    except Exception:
        logging.exception("Failed to populate 'courses_professors' and 'courses_teachingassistants' tables.")
        raise


def read_professors_and_TAs(print_df: bool = False) -> Tuple[pd.DataFrame, pd.DataFrame]:
    """
    read_professors_and_TAs: Reads and returns the contents of the 'courses_professors' and 'courses_teachingassistants' tables as Pandas DataFrames.

    Converts all columns to int64 for consistency and optionally prints
    the DataFrames for inspection.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the loaded DataFrames and their data types. Default is False.

    Returns
    -------
    df_professors: pd.DataFrame
            A DataFrame containing professor records from the database.

            Columns:
                - id ('int64'): Primary key
                - user_id ('int64'): Foreign key referencing 'users_user.id'
    df_TAs: pd.DataFrame
            A DataFrame containing teaching assistant records from the database.

            Columns:
                - id ('int64'): Primary key
                - user_id ('int64'): Foreign key referencing 'users_user.id'
    """
    try:
        # Load courses_professors and courses_teachingassistants into dataframes
        df_professors = pd.read_sql("SELECT * FROM courses_professors", ENGINE)
        df_TAs = pd.read_sql("SELECT * FROM courses_teachingassistants", ENGINE)

        # Alter column data types
        df_professors[df_professors.columns] = df_professors[df_professors.columns].astype("int64")
        df_TAs[df_TAs.columns] = df_TAs[df_TAs.columns].astype("int64")

        # Print dataframes
        if print_df:
            print(f"df_professors:\n{df_professors}\n{df_professors.dtypes}")
            print(f"df_TAs:\n{df_TAs}\n{df_TAs.dtypes}")

        return df_professors, df_TAs

    except Exception:
        logging.exception("Failed to read 'courses_professors' and 'courses_teachingassistants' tables.")
        raise


def populate_course_catalog() -> None:
    """
    populate_course_catalog: Populates the 'courses_coursecatalog' table with course metadata from CSV.

    Loads data from 'course_catalog.csv' located at BASE_PATH,
    casts relevant fields to appropriate types, and inserts the
    data into the database.

    Parameters
    ----------
    None

    Returns
    -------
    None

    Affected Table
    --------------
    courses_coursecatalog

    """
    try:
        # Column names and types
        smallint_columns = ["catalog_number"]
        string_columns = ["name", "subject", "course_title", "course_level"]

        # Load csv data into dataframe
        df_catalog = pd.read_csv(BASE_PATH / "course_catalog.csv")

        # Alter column data types
        df_catalog[smallint_columns] = df_catalog[smallint_columns].astype("int16")
        df_catalog[string_columns] = df_catalog[string_columns].astype(str)

        # Write dataframe contents to database
        df_catalog.to_sql("courses_coursecatalog", ENGINE, if_exists="append", index=False)

        logging.info("Successfully populated 'courses_coursecatalog' table.")

    except Exception:
        logging.exception("Failed to populate 'courses_coursecatalog' table.")
        raise


def read_course_catalog(print_df: bool = False) -> pd.DataFrame:
    """
    read_course_catalog: Reads and returns the contents of the 'courses_coursecatalog' table as a Pandas DataFrame.

    Converts all columns to appropriate types for consistency and optionally prints
    the DataFrame for inspection.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the loaded DataFrame and its data types. Default is False.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing course catalog records from the database.

        Columns:
            - id (int64): Primary key
            - name (str): Course code name
            - subject (str): Subject area
            - catalog_number (int16): Catalog number
            - course_title (str): Full course title
            - course_level (str): Level descriptor
    """
    try:
        # Column names and types
        bigint_columns = ["id"]
        smallint_columns = ["catalog_number"]
        string_columns = ["name", "subject", "course_title", "course_level"]

        # Load courses_coursecatalog table into a dataframe
        df_catalog = pd.read_sql("SELECT * FROM courses_coursecatalog", ENGINE)

        # Alter column types
        df_catalog[bigint_columns] = df_catalog[bigint_columns].astype("int64")
        df_catalog[smallint_columns] = df_catalog[smallint_columns].astype("int16")
        df_catalog[string_columns] = df_catalog[string_columns].astype(str)

        # Print dataframes
        if print_df:
            print(f"df_catalog:\n{df_catalog}\n{df_catalog.dtypes}")

        return df_catalog

    except Exception:
        logging.exception("Failed to read 'courses_coursecatalog'")
        raise


def populate_courses_semester() -> None:
    """
    populate_courses_semester: Populates the 'courses_semester' table with semester metadata.

    Loads data from 'courses_semester.csv' located at BASE_PATH, casts
    relevant fields to appropriate types, and inserts the records into
    the database.

    Parameters
    ----------
    None

    Returns
    -------
    None

    Affected Table
    --------------
    - courses_semester
    """
    try:
        # Column names and types
        smallint_columns = ["year"]
        string_columns = ["name", "term", "session"]

        # Load csv data into a dataframe
        df_semester = pd.read_csv(BASE_PATH / "semesters.csv", keep_default_na=False)

        # Alter column data types
        df_semester[smallint_columns] = df_semester[smallint_columns].astype("int16")
        df_semester[string_columns] = df_semester[string_columns].astype(str)

        # Append dataframe's contents to database table
        df_semester.to_sql("courses_semester", ENGINE, if_exists="append", index=False)

        logging.info("Successfully populated 'courses_semester' table.")

    except Exception:
        logging.exception("Failed to populate 'courses_semester' table.")
        raise


def read_courses_semester(print_df: bool = False) -> pd.DataFrame:
    """
    read_courses_semester: Reads and returns the contents of the 'courses_semester' table as a Pandas DataFrame.

    Converts all columns to their appropriate types and optionally prints the DataFrame
    for inspection.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the loaded DataFrame and its data types. Default is False.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing semester metadata from the database.

        Columns:
            - id (int64): Primary key
            - name (str): Human-readable name of the semester
            - year (int16): Academic year
            - term (str): Academic term
            - session (str): Session type
    """
    try:
        # Column names and types
        bigint_columns = ["id"]
        smallint_columns = ["year"]
        string_columns = ["name", "term", "session"]

        # Load courses_semester table into a dataframe
        df_semester = pd.read_sql("SELECT * FROM courses_semester", ENGINE)

        # Alter column types
        df_semester[bigint_columns] = df_semester[bigint_columns].astype("int64")
        df_semester[smallint_columns] = df_semester[smallint_columns].astype("int16")
        df_semester[string_columns] = df_semester[string_columns].astype(str)

        # Print dataframes
        if print_df:
            print(f"df_semester:\n{df_semester}\n{df_semester.dtypes}")

        return df_semester

    except Exception:
        logging.exception("Failed to read 'courses_semester'")
        raise


def populate_course_instances() -> None:
    """
    populate_course_instances: Populates the 'courses_courseinstances' table with metadata for course sections.

    This function reads from 'course_instances.csv', resolves foreign key relationships
    for catalog, semester, professor, and teaching assistant assignments, and appends
    the resulting records into the database.

    Parameters
    ----------
    None

    Returns
    -------
    None

    Affected Table
    --------------
    - courses_courseinstances
    """
    try:
        # Column names and types
        bigint_columns = ["canvas_course_id"]
        int_columns = ["section_number"]
        smallint_columns = ["year"]
        string_columns = ["course_name", "term", "session"]

        drop_columns = ["course_name", "year", "term", "session"]

        # Load csv data into a dataframe
        df_course_instances = pd.read_csv(BASE_PATH / "course_instances.csv", keep_default_na=False)

        # Alter column data types
        df_course_instances[smallint_columns] = df_course_instances[smallint_columns].astype("int16")
        df_course_instances[int_columns] = df_course_instances[int_columns].astype("int32")
        df_course_instances[bigint_columns] = df_course_instances[bigint_columns].astype("int64")
        df_course_instances[string_columns] = df_course_instances[string_columns].astype(str)

        # Read tables into dataframes
        df_catalog = read_course_catalog()
        df_semester = read_courses_semester()
        df_professors, df_TAs = read_professors_and_TAs()

        # Dataframe processing
        df_course_instances["course_catalog_id"] = pd.Series([-1] * len(df_course_instances), dtype="int64")
        df_course_instances["semester_id"] = pd.Series([-1] * len(df_course_instances), dtype="int64")
        df_course_instances["professor_id"] = pd.Series([-1] * len(df_course_instances), dtype="int64")
        df_course_instances["teaching_assistant_id"] = pd.Series([-1] * len(df_course_instances), dtype="int64")

        for iter, row in df_course_instances.iterrows():
            df_course_instances.loc[df_course_instances["canvas_course_id"] == row["canvas_course_id"], "course_catalog_id"] = df_catalog[df_catalog["name"] == row["course_name"]]["id"].iloc[0]
            df_course_instances.loc[df_course_instances["canvas_course_id"] == row["canvas_course_id"], "semester_id"] = df_semester[(df_semester["year"] == row["year"]) & (df_semester["term"] == row["term"]) & (df_semester["session"] == row["session"])]["id"].iloc[0]
            df_course_instances.loc[df_course_instances["canvas_course_id"] == row["canvas_course_id"], "professor_id"] = df_professors[df_professors["id"] == (iter % (len(df_professors)) + 1)]["id"].iloc[0]
            df_course_instances.loc[df_course_instances["canvas_course_id"] == row["canvas_course_id"], "teaching_assistant_id"] = df_TAs[df_TAs["id"] == (iter % (len(df_TAs)) + 1)]["id"].iloc[0]

        df_course_instances.drop(drop_columns, axis=1, inplace=True)

        # Append dataframe's contents to database table
        df_course_instances.to_sql("courses_courseinstances", ENGINE, if_exists="append", index=False)

        logging.info("Successfully populated 'courses_courseinstance' table.")

    except Exception:
        logging.exception("Failed to populate 'courses_courseinstance' table.")
        raise


def read_course_instances(print_df: bool = False) -> pd.DataFrame:
    """
    read_course_instances: Reads and returns the contents of the 'courses_courseinstances' table as a Pandas DataFrame.

    Converts all columns to appropriate types and optionally prints the DataFrame
    and its data types for inspection.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the loaded DataFrame and its data types. Default is False.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing course instance records from the database.

        Columns:
            - id (int64): Primary key
            - section_number (int32): Course section identifier
            - canvas_course_id (int64): Unique Canvas course ID
            - course_catalog_id (int64): Foreign key referencing 'courses_coursecatalog.id'
            - semester_id (int64): Foreign key referencing 'courses_semester.id'
            - professor_id (int64): Foreign key referencing 'courses_professors.id'
            - teaching_assistant_id (int64): Foreign key referencing 'courses_teachingassistants.id'
    """
    try:
        # Column names and types
        bigint_columns = ["id", "canvas_course_id", "course_catalog_id", "semester_id", "professor_id", "teaching_assistant_id"]
        int_columns = ["section_number"]

        # Load courses_courseinstances table into a dataframe
        df_course_instances = pd.read_sql("SELECT * FROM courses_courseinstances", ENGINE)

        # Alter column data types
        df_course_instances[int_columns] = df_course_instances[int_columns].astype("int32")
        df_course_instances[bigint_columns] = df_course_instances[bigint_columns].astype("int64")

        # Print dataframes
        if print_df:
            print(f"df_course_instances:\n{df_course_instances}\n{df_course_instances.dtypes}")

        return df_course_instances

    except Exception:
        logging.exception("Failed to read 'courses_courseinstances'")
        raise


def populate_professor_and_TAs_enrollment() -> None:
    """
    populate_professor_and_TAs_enrollment: Populates the 'courses_professorenrollments' and 'courses_teachingassistantenrollments' tables.

    This function reads the 'courses_courseinstances' table, extracts the assigned
    professor and teaching assistant IDs, and writes the enrollment relationships
    to their respective tables.

    Parameters
    ----------
    None

    Returns
    -------
    None

    Affected Tables
    ---------------
    - courses_professorenrollments
    - courses_teachingassistantenrollments

    """
    try:
        # Load courses_courseinstances table into a dataframe
        df_course_instances = read_course_instances()

        # Dataframe processing
        df_enrollments = df_course_instances[["id", "professor_id", "teaching_assistant_id"]].rename(columns={"id": "course_instance_id"})

        # Append each dataframe's contents to their respective tables in the database
        df_enrollments[["course_instance_id", "professor_id"]].to_sql("courses_professorenrollments", ENGINE, if_exists="append", index=False)
        df_enrollments[["course_instance_id", "teaching_assistant_id"]].to_sql("courses_teachingassistantenrollments", ENGINE, if_exists="append", index=False)

        logging.info("Successfully populated 'courses_professorenrollments' and 'courses_teachingassistantenrollments' tables.")

    except Exception:
        logging.exception("Failed to populate 'courses_professorenrollments' and 'courses_teachingassistantenrollments' tables.")
        raise


def read_professors_and_TAs_enrollment(print_df: bool = False) -> pd.DataFrame:
    """
    read_professors_and_TAs_enrollment: Reads and returns the contents of the 'courses_professorenrollments' and 'courses_teachingassistantenrollments' tables as Pandas DataFrames.

    Converts all columns to int64 for consistency and optionally prints
    the DataFrames for inspection.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the loaded DataFrames and their data types. Default is False.

    Returns
    -------
    df_professors_enrollments: pd.DataFrame
        A DataFrame containing professor-to-course enrollment records.

        Columns:
            - id (int64): Primary key
            - course_instance_id (int64): Foreign key referencing 'courses_courseinstances.id'
            - professor_id (int64): Foreign key referencing 'courses_professors.id'

    df_TAs_enrollments: pd.DataFrame
        A DataFrame containing teaching assistant-to-course enrollment records.

        Columns:
            - id (int64): Primary key
            - course_instance_id (int64): Foreign key referencing 'courses_courseinstances.id'
            - teaching_assistant_id (int64): Foreign key referencing 'courses_teachingassistants.id'
    """
    try:
        # Load courses_professorsenrollments and courses_teachingassistantsenrollments into dataframes
        df_professors_enrollments = pd.read_sql("SELECT * FROM courses_professorenrollments", ENGINE)
        df_TAs_enrollments = pd.read_sql("SELECT * FROM courses_teachingassistantenrollments", ENGINE)

        # Alter column data types
        df_professors_enrollments[df_professors_enrollments.columns] = df_professors_enrollments[df_professors_enrollments.columns].astype("int64")
        df_TAs_enrollments[df_TAs_enrollments.columns] = df_TAs_enrollments[df_TAs_enrollments.columns].astype("int64")

        # Print dataframes
        if print_df:
            print(f"df_professors_enrollments:\n{df_professors_enrollments}\n{df_professors_enrollments.dtypes}")
            print(f"df_TAs_enrollments:\n{df_TAs_enrollments}\n{df_TAs_enrollments.dtypes}")

        return df_professors_enrollments, df_TAs_enrollments

    except Exception:
        logging.exception("Failed to read 'courses_professorenrollments' and 'courses_teachingassistantenrollments'")
        raise


def populate_course_student_and_enrollments() -> None:
    """
    populate_course_student_and_enrollments: Populates the 'courses_students' and 'courses_studentenrollments' tables.

    This function loads fabricated student records from CSV, maps each student
    to a course instance using the Canvas course ID, and inserts the data into
    the student and enrollment tables accordingly.

    Parameters
    ----------
    None

    Returns
    -------
    None

    Affected Tables
    ---------------
    - courses_students
    - courses_studentenrollments
    """
    try:
        # Column names and types
        bigint_columns = ["nshe_id", "codeGrade_id", "canvas_course_id"]
        string_columns = ["first_name", "last_name", "ace_id", "email"]

        # Load csv data into a dataframe
        df_students = pd.read_csv(BASE_PATH / "students.csv", keep_default_na=False)

        # Load courses_courseinstancesr table into a dataframe
        df_course_instances = read_course_instances()

        # Alter column data types
        df_students[bigint_columns] = df_students[bigint_columns].astype("int64")
        df_students[string_columns] = df_students[string_columns].astype(str)

        # Left join on "canvas_course_id" to tie students to their course_instance_id
        df_students = df_students.merge(df_course_instances[["canvas_course_id", "id"]], how="left", on="canvas_course_id").rename(columns={"id": "course_instance_id"})

        # Append dataframe's contents to database table
        df_students.drop(['canvas_course_id', 'course_instance_id'], axis=1).to_sql("courses_students", ENGINE, if_exists="append", index=False)

        logging.info("Successfully populated 'courses_students' table.")

        # Pull recently uploaded data from database for the increment id value
        df_students_from_database = pd.read_sql("SELECT * FROM courses_students", ENGINE)

        # Left join on "nshe_id" to tie student_ids to course_instance_ids
        df_studentenrollments = df_students_from_database.merge(df_students[["nshe_id", "course_instance_id"]], how="left", on="nshe_id").rename(columns={"id": "student_id"})

        # Append dataframe's contents to database table
        df_studentenrollments[["student_id", "course_instance_id"]].to_sql("courses_studentenrollments", ENGINE, if_exists="append", index=False)

        logging.info("Successfully populated 'courses_studentenrollments' table.")

    except Exception:
        logging.exception("Failed to populate 'courses_students' or 'courses_studentenrollments' tables.")
        raise


def read_course_student(print_df: bool = False) -> pd.DataFrame:
    """
    read_course_student: Reads and returns the contents of the 'courses_students' table as a Pandas DataFrame.

    Converts all columns to appropriate types and optionally prints the DataFrame
    and its data types for inspection.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the loaded DataFrame and its data types. Default is False.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing student records from the database.

        Columns:
            - id (int64): Primary key
            - first_name (str)
            - last_name (str)
            - ace_id (str)
            - email (str)
            - nshe_id (int64)
            - codeGrade_id (int64)
    """
    try:
        # Column names and types
        bigint_columns = ["id", "nshe_id", "codeGrade_id"]
        string_columns = ["first_name", "last_name", "ace_id", "email"]

        # Load courses_students into a dataframe
        df_students = pd.read_sql("SELECT * FROM courses_students", ENGINE)

        # Alter column data types
        df_students[bigint_columns] = df_students[bigint_columns].astype("int64")
        df_students[string_columns] = df_students[string_columns].astype(str)

        # Print dataframe
        if print_df:
            print(f"df_students:\n{df_students}\n{df_students.dtypes}")

        return df_students

    except Exception:
        logging.exception("Failed to read 'courses_students'")
        raise


def read_course_student_enrollments(print_df: bool = False) -> pd.DataFrame:
    """
    read_course_student_enrollments: Reads and returns the contents of the 'courses_studentenrollments' table as a Pandas DataFrame.

    Converts all columns to int64 for consistency and optionally prints
    the DataFrame and its data types for inspection.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the loaded DataFrame and its data types. Default is False.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing student-to-course enrollment records from the database.

        Columns:
            - id (int64): Primary key
            - student_id (int64): Foreign key referencing 'courses_students.id'
            - course_instance_id (int64): Foreign key referencing 'courses_courseinstances.id'
    """
    try:
        # Load courses_studentenrollments into a dataframe
        df_student_enrollments = pd.read_sql("SELECT * FROM courses_studentenrollments", ENGINE)

        # Alter column data types
        df_student_enrollments[df_student_enrollments.columns] = df_student_enrollments[df_student_enrollments.columns].astype("int64")

        # Print dataframe
        if print_df:
            print(f"df_student_enrollments:\n{df_student_enrollments}\n{df_student_enrollments.dtypes}")

        return df_student_enrollments

    except Exception:
        logging.exception("Failed to read 'courses_studentenrollments'")
        raise


def populate_assignments() -> None:
    """
    populate_assignments: Populates the 'assignments_assignments' table and creates corresponding assignment directories, MOSS report folders, and placeholder PDF files.

    This function reads from 'assignments.csv', resolves foreign keys for
    semester and course catalog, generates file paths, creates directories,
    and inserts formatted assignment records into the database.

    Parameters
    ----------
    None

    Returns
    -------
    None

    Affected Table
    --------------
    - assignments_assignments
    """
    try:
        # Set paths
        assignment_dir = BASE_PATH_PRISM / "assignments"
        pdf_src_path = BASE_PATH / "test.pdf"

        # Column names and types
        boolean_columns = ['has_base_code', 'has_policy']
        date_columns = ['due_date', 'lock_date']
        smallint_columns = ['year', 'catalog_number', 'assignment_number']
        string_columns = ['subject', 'term', 'title', 'language', 'session']

        # Load csv data into a dataframe
        df_assignments = pd.read_csv(BASE_PATH / "assignments.csv", keep_default_na=False)

        # Alter column data types
        df_assignments[boolean_columns] = df_assignments[boolean_columns].astype(bool)
        for col in date_columns:
            df_assignments[col] = pd.to_datetime(df_assignments[col], utc=True)
        df_assignments[smallint_columns] = df_assignments[smallint_columns].astype("int16")
        df_assignments[string_columns] = df_assignments[string_columns].astype(str)

        # Read tables in dataframes
        df_semester = read_courses_semester()
        df_course_catalog = read_course_catalog()

        # Merge with semester and catalog data to assign foreign keys
        df_assignments = df_assignments.merge(df_semester[['id', 'term', 'year', 'session']].rename(columns={"id": "semester_id"}), how="left", on=['term', 'year', 'session'])
        df_assignments = df_assignments.merge(df_course_catalog[['id', 'subject', 'catalog_number']].rename(columns={"id": "course_catalog_id"}), how="left", on=['subject', 'catalog_number'])

        # Drop unused descriptive columns
        df_assignments.drop(['term', 'year', 'session', 'subject', 'catalog_number'], axis=1, inplace=True)

        # Insert new column with default value
        df_assignments["moss_report_directory_path"] = pd.Series([""] * len(df_assignments), dtype=str)
        df_assignments["bulk_ai_directory_path"] = pd.Series([""] * len(df_assignments), dtype=str)
        df_assignments["pdf_filepath"] = pd.Series([""] * len(df_assignments), dtype=str)

        # Retrieve the next interger in the identifier sequence for 'assignments_assignments'
        assignment_id = -1
        if pd.read_sql("SELECT * FROM pg_sequences WHERE schemaname = 'public' AND sequencename = 'assignments_assignments_id_seq';", ENGINE)["last_value"].iloc[0] is None:
            assignment_id = 1
        else:
            assignment_id = pd.read_sql("SELECT last_value FROM assignments_assignments_id_seq;", ENGINE).iloc[0, 0]

        # Generate file and directory names, and write to local filesystem
        for iter in df_assignments.index:
            moss_path = assignment_dir / f"assignment_{assignment_id}" / "moss_reports"
            ai_path = assignment_dir / f"assignment_{assignment_id}" / "ai_submissions"
            pdf_dest_path = assignment_dir / f"assignment_{assignment_id}" / f"assignment_{df_assignments.loc[iter, "assignment_number"]}.pdf"

            df_assignments.loc[iter, "moss_report_directory_path"] = str(moss_path)
            df_assignments.loc[iter, "bulk_ai_directory_path"] = str(ai_path)
            df_assignments.loc[iter, "pdf_filepath"] = str(pdf_dest_path)

            moss_path.mkdir(parents=True, exist_ok=True)
            ai_path.mkdir(parents=True, exist_ok=True)
            shutil.copy(pdf_src_path, pdf_dest_path)

            assignment_id += 1

        # Append dataframe's contents to database table
        df_assignments.to_sql("assignments_assignments", ENGINE, if_exists="append", index=False)

        logging.info("Successfully populated 'assignments_assignments' table.")

    except Exception:
        logging.exception("Failed to populate 'assignments_assignments' table.")
        raise


def read_assignments(print_df: bool = False) -> pd.DataFrame:
    """
    read_assignments: Reads and returns the contents of the 'assignments_assignments' table as a Pandas DataFrame.

    Converts all columns to appropriate types and optionally prints the DataFrame
    and its data types for inspection.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the loaded DataFrame and its data types. Default is False.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing assignment records from the database.

        Columns:
            - id (int64): Primary key
            - course_catalog_id (int64): Foreign key referencing referencing 'courses_coursecatalog.id'
            - semester_id (int64): Foreign key referencing referencing 'courses_semester.id'
            - assignment_number (int16): Numeric identifier for the assignment
            - title (str): Assignment title
            - due_date (datetime): Submission due date
            - lock_date (datetime): Late submission lock date
            - has_base_code (bool): Whether base code is provided
            - has_policy (bool): Whether a policy was attached
            - language (str): Programming language
            - pdf_filepath (str): Filepath to the assignment PDF
            - moss_report_directory_path (str): Directory for MOSS output
            - bulk_ai_directory_path (str): Directory for AI-generated submissions
    """
    try:
        # Column names and types
        boolean_columns = ["has_base_code", "has_policy"]
        bigint_columns = ["id", "course_catalog_id", "semester_id"]
        date_columns = ["due_date", "lock_date"]
        smallint_columns = ["assignment_number"]
        string_columns = ["title", "pdf_filepath", "moss_report_directory_path", "bulk_ai_directory_path", "language"]

        # Load assignments_assignments into a dataframe
        df_assignments = pd.read_sql("SELECT * FROM assignments_assignments", ENGINE)

        # Alter column data types
        df_assignments[boolean_columns] = df_assignments[boolean_columns].astype(bool)
        df_assignments[bigint_columns] = df_assignments[bigint_columns].astype("int64")
        for col in date_columns:
            df_assignments[col] = pd.to_datetime(df_assignments[col])
        df_assignments[smallint_columns] = df_assignments[smallint_columns].astype("int16")
        df_assignments[string_columns] = df_assignments[string_columns].astype(str)

        # Print dataframe
        if print_df:
            print(f"df_assignments:\n{df_assignments}\n{df_assignments.dtypes}")

        return df_assignments

    except Exception:
        logging.exception("Failed to read 'assignments_assignments'")
        raise


def parseFileName(input: str) -> list[str]:
    """
    parseFileName: Parses a standardized assignment filename to extract metadata components.

    Expected format:
        "SUBJECT CATALOG_NUMBER COURSE_SECTION - YEAR TERM - Assignment ASSIGNMENT_NUMBER"

    Example:
        "CS 135 1001 - 2024 Sprg - Assignment 1"

    Parameters
    ----------
    input : str
        The filename or path string from which to extract metadata.

    Returns
    -------
    list[str]
        A list containing:
            [subject, catalog_number, course_section, year, term, assignment_number]
    """
    try:
        term = {"Sprg": "Spring", "Sumr": "Summer", "Fall": "Fall"}

        parts = input.split(" - ")
        course_info = parts[0].split(" ")
        semester_info = parts[1].split(" ")
        semester_info[1] = term[semester_info[1]]

        return course_info + semester_info + parts[2:]

    except Exception:
        logging.exception(f"Failed to parse: '{input}'")
        raise


def gen_path(row: pd.Series) -> Path:
    """
    gen_path: Generates the destination path for unzipped student submissions.

    Constructs a standardized path using the assignment ID and the base name
    of the original ZIP file (with spaces replaced by underscores).

    Example:
        If row["assignment_id"] = 5 and row["path"] = "CS 135 1001 - Assignment 1.zip",
        the resulting path will be:
            /PRISM/data/assignments/assignment_5/bulk_submission/CS_135_1001_-_Assignment_1

    Parameters
    ----------
    row : pd.Series
        A DataFrame row with 'path' and 'assignment_id' columns.

    Returns
    -------
    pathlib.Path
        Full path to where the contents of the ZIP file should be extracted.
    """
    try:
        dest_dir_path = BASE_PATH_PRISM / "assignments"
        return dest_dir_path / f"assignment_{row["assignment_id"]}" / "bulk_submission" / str(Path(row["path"]).stem).replace(" ", "_")

    except Exception:
        logging.exception(f"Failed to generate a path using the arguments: {row.to_dict()}")
        raise


def unzip_and_copy_bulk_assignment_files(row: pd.Series) -> None:
    """
    unzip_and_copy_bulk_assignment_files: Unzips and copies the contents of a bulk assignment ZIP file to its destination directory.

    Also copies the corresponding CSV metadata file (with .csv extension) and renames
    any extracted files to replace spaces with underscores for consistency.

    Expected columns in `row`:
        - path: str or Path to the original ZIP file (excluding extension)
        - dest_path: str or Path to the destination directory

    Parameters
    ----------
    row : pd.Series
        A row containing 'path' and 'dest_path' used to locate and copy the bulk submission.

    Returns
    -------
    None
    """
    try:
        # Ensure paths are of type Path
        path = Path(row["path"])
        dest_path = Path(row["dest_path"])

        # Create destination directory if it doesn't exist
        dest_path.mkdir(parents=True, exist_ok=True)

        # Copy the corresponding CSV file to the destination (renamed to avoid spaces)
        shutil.copy(path.with_suffix(".csv"), Path(str(dest_path.with_suffix(".csv")).replace(" ", "_")))

        # Extract the ZIP file contents to the destination directory
        with zipfile.ZipFile(path.with_suffix(".zip"), 'r') as zip_ref:
            zip_ref.extractall(dest_path)

        # Sanitize filenames by replacing spaces with underscores
        for item in dest_path.iterdir():
            name = item.name.replace(" ", "_")
            item.rename(item.parent / name)

    except Exception:
        logging.exception(f"Failed to process row: {row.to_dict()}")
        raise


def populate_filesystem_and_bulk_submissions(write_files: bool = False) -> None:
    """
    populate_filesystem_and_bulk_submissions: Populates the 'assignments_bulksubmissions' and 'courses_courseassignmentcollaboration' tables.

    This function parses assignment ZIP filenames to extract metadata, joins it with existing
    database records to resolve foreign keys, generates destination directories, optionally
    extracts files, and writes the structured data to the database.

    Parameters
    ----------
    write_files : bool, optional
        If True, unzips the bulk submission archives and copies the associated metadata files
        to their generated destination folders.

    Returns
    -------
    None

    Affected Tables
    ---------------
    - assignments_bulksubmissions
    - courses_courseassignmentcollaboration

    Notes
    -----
    - Only Spring and Fall assignments are processed; Summer assignments are excluded.
    - This function assumes that filenames follow the standard format used by CodeGrade exports.
    """
    try:
        src_path_dir = BASE_PATH_PRISM / "final"

        # Lists for storing parsed metadata
        rows = []
        columns = ["subject", "catalog_number", "section_number", "year", "term", "title", "path"]

        # Column names and types
        smallint_columns = ["catalog_number", "section_number", "year"]
        string_columns = ["subject", "term", "title"]

        # While traverse the source directory, collect and parse the stems of the zip files found
        for file_path in src_path_dir.iterdir():
            if file_path.is_file() and file_path.suffix == ".zip":
                rows.append(parseFileName(file_path.stem) + [str(file_path.with_suffix(''))])

        # Convert lists to dataframe for processing
        df_bulksubmissions = pd.DataFrame(rows, columns=columns)

        # Drop Summer assignments due to unrealistic due-date scheduling
        df_bulksubmissions = df_bulksubmissions[df_bulksubmissions["term"] != "Summer"].reset_index(drop=True)

        # Alter column data types
        df_bulksubmissions[smallint_columns] = df_bulksubmissions[smallint_columns].astype("int16")
        df_bulksubmissions[string_columns] = df_bulksubmissions[string_columns].astype(str)

        # Read tables in dataframes
        df_semester = read_courses_semester()
        df_course_catalog = read_course_catalog()
        df_course_instances = read_course_instances()
        df_assignments = read_assignments()

        # Merge metadata to resolve IDs
        df_bulksubmissions = df_bulksubmissions.merge(df_semester[["id", "year", "term"]].rename(columns={"id": "semester_id"}), how="left", on=["year", "term"]).drop(["year", "term"], axis=1)
        df_bulksubmissions = df_bulksubmissions.merge(df_course_catalog[["id", "subject", "catalog_number"]].rename(columns={"id": "course_catalog_id"}), how="left", on=["subject", "catalog_number"]).drop(["subject", "catalog_number"], axis=1)
        df_bulksubmissions = df_bulksubmissions.merge(df_course_instances[["id", "section_number", "course_catalog_id", "semester_id"]].rename(columns={"id": "course_instance_id"}), how="left", on=["section_number", "course_catalog_id", "semester_id"]).drop(["section_number"], axis=1)
        df_bulksubmissions = df_bulksubmissions.merge(df_assignments[["id", "title", "course_catalog_id", "semester_id"]].rename(columns={"id": "assignment_id"}), how="left", on=["title", "course_catalog_id", "semester_id"]).drop(["title", "course_catalog_id", "semester_id"], axis=1)

        # Using the assignment id, generate destination paths for the unzip
        df_bulksubmissions["dest_path"] = df_bulksubmissions[["path", "assignment_id"]].apply(gen_path, axis=1)

        # Unzip and copy files from the source directory to the destination directory
        if write_files:
            df_bulksubmissions[["path", "dest_path"]].apply(unzip_and_copy_bulk_assignment_files, axis=1)

        # Final clean-up and formatting
        df_bulksubmissions = df_bulksubmissions.drop("path", axis=1).rename(columns={"dest_path": "directory_path"}).sort_values(["assignment_id", "course_instance_id"]).reset_index(drop=True)

        # Covert from PosixPath to string
        df_bulksubmissions["directory_path"] = df_bulksubmissions["directory_path"].astype(str)

        # Append dataframe's contents to database table
        df_bulksubmissions.to_sql("assignments_bulksubmissions", ENGINE, if_exists="append", index=False)
        logging.info("Successfully populated 'assignments_bulksubmissions' table.")

    except Exception:
        logging.exception("Failed to populate 'assignments_bulksubmissions' or 'courses_courseassignmentcollaboration' tables.")
        raise


def read_bulksubmissions(print_df: bool = False) -> pd.DataFrame:
    """
    read_bulksubmissions: Reads and returns the contents of the 'assignments_bulksubmissions' table as a Pandas DataFrame.

    Converts all columns to appropriate types and optionally prints the DataFrame
    and its data types for inspection.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the loaded DataFrame and its data types. Default is False.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing bulk submission directory metadata.

        Columns:
            - id (int64): Primary key
            - assignment_id (int64): Foreign key referencing 'assignments_assignments.id'
            - course_instance_id (int64): Foreign key referencing 'courses_courseinstances.id'
            - directory_path (str): File system path to extracted student submission directory
    """
    try:
        # Column names and types
        bigint_columns = ["id", "assignment_id", "course_instance_id"]
        string_columns = ["directory_path"]

        # Load assignments_bulksubmissions table into a dataframe
        df_bulksubmissions = pd.read_sql("SELECT * FROM assignments_bulksubmissions", ENGINE)

        # Alter column data types
        df_bulksubmissions[bigint_columns] = df_bulksubmissions[bigint_columns].astype("int64")
        df_bulksubmissions[string_columns] = df_bulksubmissions[string_columns].astype(str)

        # Print dataframes
        if print_df:
            print(f"df_bulksubmissions:\n{df_bulksubmissions}\n{df_bulksubmissions.dtypes}")

        return df_bulksubmissions

    except Exception:
        logging.exception("Failed to read 'assignments_bulksubmissions'")
        raise


def populate_requiredsubmissionfiles() -> None:
    """
    populate_requiredsubmissionfiles: Populates the 'assignments_requiredsubmissionfiles' table with expected filenames and thresholds.

    For each assignment in the system, this function inserts a default required submission file
    (main.cpp) with a default similarity threshold of 60.00%.

    Parameters
    ----------
    None

    Returns
    -------
    None

    Affected Table
    --------------
    - assignments_requiredsubmissionfiles
    """
    try:
        # Initialize dataframe
        df_requiredsubmissionfiles = pd.DataFrame()

        # Gather all assignment ids
        df_requiredsubmissionfiles["assignment_id"] = read_assignments()["id"]

        # For the plagarism dataset, there are only main.cpp files
        df_requiredsubmissionfiles["file_name"] = pd.Series(["main.cpp"] * len(df_requiredsubmissionfiles), dtype=str)

        # Default similarity threshold set at 60.00%
        df_requiredsubmissionfiles["similarity_threshold"] = pd.Series([60.00] * len(df_requiredsubmissionfiles), dtype="float64")

        # Append dataframe's contents to database table
        df_requiredsubmissionfiles.to_sql("assignments_requiredsubmissionfiles", ENGINE, if_exists="append", index=False)
        logging.info("Successfully populated 'assignments_requiredsubmissionfiles' table.")

    except Exception:
        logging.exception("Failed to populate 'assignments_requiredsubmissionfiles' table.")


def read_requiredsubmissionfiles(print_df: bool = False) -> pd.DataFrame:
    """
    read_requiredsubmissionfiles: Reads and returns the contents of the 'assignments_requiredsubmissionfiles' table as a Pandas DataFrame.

    Converts all columns to appropriate types and optionally prints the DataFrame
    and its data types for inspection.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the loaded DataFrame and its data types. Default is False.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing required file metadata for each assignment.

        Columns:
            - id (int64): Primary key
            - assignment_id (int64): FK referencing 'assignments_assignments.id'
            - file_name (str): Expected name of the required submission file
            - similarity_threshold (float64): Minimum similarity percentage to be flagged
    """
    try:
        # Column names and types
        bigint_columns = ["id", "assignment_id"]
        float_columns = ["similarity_threshold"]
        string_columns = ["file_name"]

        # Load assignments_requiredsubmissionfiles table into a dataframe
        df_requiredsubmissionfiles = pd.read_sql("SELECT * FROM assignments_requiredsubmissionfiles", ENGINE)

        # Alter column data types
        df_requiredsubmissionfiles[bigint_columns] = df_requiredsubmissionfiles[bigint_columns].astype("int64")
        df_requiredsubmissionfiles[float_columns] = df_requiredsubmissionfiles[float_columns].astype("float64")
        df_requiredsubmissionfiles[string_columns] = df_requiredsubmissionfiles[string_columns].astype(str)

        # Print dataframes
        if print_df:
            print(f"df_requiredsubmissionfiles:\n{df_requiredsubmissionfiles}\n{df_requiredsubmissionfiles.dtypes}")

        return df_requiredsubmissionfiles

    except Exception:
        logging.exception("Failed to read 'assignments_requiredsubmissionfiles'")
        raise


def read_basefiles(print_df: bool = False) -> pd.DataFrame:
    """
    read_basefiles: Reads and returns the contents of the 'assignments_basefiles' table as a Pandas DataFrame.

    Converts all columns to appropriate types and optionally prints the DataFrame
    and its data types for inspection.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the loaded DataFrame and its data types. Default is False.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing base code file records linked to assignments.

        Columns:
            - id (int64): Primary key
            - assignment_id (int64): FK referencing 'assignments_assignments.id'
            - file_name (str): Name of the base file
            - file_path (str): Path to the base file on disk
    """
    try:
        # Column names and types
        bigint_columns = ["id", "assignment_id"]
        string_columns = ["file_name", "file_path"]

        # Load assignments_basefiles table into a dataframe
        df_basefiles = pd.read_sql("SELECT * FROM assignments_basefiles", ENGINE)

        # Alter column data types
        df_basefiles[bigint_columns] = df_basefiles[bigint_columns].astype("int64")
        df_basefiles[string_columns] = df_basefiles[string_columns].astype(str)

        # Print dataframes
        if print_df:
            print(f"df_basefiles:\n{df_basefiles}\n{df_basefiles.dtypes}")

        return df_basefiles

    except Exception:
        logging.exception("Failed to read 'assignments_basefiles'")
        raise


def clear_directory(directory: Path):
    """
    clear_directory: Deletes all contents (files and subdirectories) within the specified directory.

    Parameters
    ----------
    directory : Path
        A pathlib.Path object representing the directory to be cleared.

    Returns
    -------
    None
    """
    try:
        for item in directory.iterdir():
            if item.is_dir():
                shutil.rmtree(item)  # Recursively delete subdirectory
            else:
                item.unlink()  # Delete file

    except Exception:
        logging.exception(f"Issue encounter when attempting to recursively delete contents of directory: {directory}'")
        raise


def generate_moss_reports() -> None:
    """
    generate_moss_reports: Generates MOSS similarity reports for all assignments in the database.

    For each assignment, this function:
      - Constructs the appropriate MOSS command
      - Clears or creates the output directory
      - Adds AI-generated files and student submissions
      - Executes the MOSS command via subprocess

    Parameters
    ----------
    None

    Returns
    -------
    None

    Notes
    -----
    - Requires MOSS executable to be configured at `MOSS_EXEC`
    - Uses directories specified in the assignment metadata
    - Ignores base code for now (reserved for future implementation)
    """
    try:
        # MOSS parameter values
        M_VALUE = 100
        N_VALUE = 100000

        # Read tables in dataframes
        df_assignments = read_assignments()
        # df_basefiles = read_basefiles()
        df_bulksubmissions = read_bulksubmissions()

        moss_command = None
        language = None
        # base_code = None
        moss_output_dir = None

        for idx in df_assignments.index:
            assignment_id = df_assignments.loc[idx, "id"]

            # Initialized the command string
            moss_command = MOSS_EXEC

            # Add language flag
            language = LANGUAGES[df_assignments.loc[idx, "language"]]
            moss_command += f" -l {language}"

            # Re/intialize output directory and add append output flag to command
            moss_output_dir = Path(df_assignments.loc[idx, "moss_report_directory_path"])
            if moss_output_dir.exists and moss_output_dir.is_dir:
                clear_directory(moss_output_dir)
            else:
                moss_output_dir.mkdir(parents=True, exist_ok=True)
            moss_command += f" -o {str(moss_output_dir)}/"

            # Base files
            if df_assignments.loc[idx, "has_base_code"]:
                pass

            # M flag
            moss_command += f" -m {M_VALUE}"

            # N flag
            moss_command += f" -n {N_VALUE}"

            # Directory flag
            moss_command += " -d"

            # Chop command string into a list
            moss_command = moss_command.split(" ")

            # Extract the paths of all ai bulk submissions folders for a given assignment id
            bulk_ai_dir = Path(df_assignments.loc[idx, "bulk_ai_directory_path"])
            if bulk_ai_dir.exists and bulk_ai_dir.is_dir and any(bulk_ai_dir.iterdir()):
                moss_command += glob.glob(f"{bulk_ai_dir}/*/*")

            # Extract the paths of all student bulk submissions folders for a given assignment id
            for dir in df_bulksubmissions[df_bulksubmissions["assignment_id"] == assignment_id]["directory_path"].apply(Path):
                moss_command += glob.glob(f"{dir}/*/*")

            # Run the MOSS command
            # logging.info(f"Executing: {MOSS_WORKING_DIRECTORY}$ {" ".join(moss_command)}")
            logging.info("Executing moss command:")
            subprocess.run(moss_command, cwd=MOSS_WORKING_DIRECTORY)

    except Exception:
        logging.exception("Failed to generate MOSS reports")
        raise


def convert_and_update(row: str, base_path: Path) -> Path:
    """
    convert_and_update: Replace spaces in a file path string and join it with a base directory.

    Parameters
    ----------
    row : str
        A relative file path or filename with potential spaces.
    base_path : Path
        The base directory to which the cleaned filename should be joined.

    Returns
    -------
    Path
        A Path object representing the new cleaned-up file path.
    """
    try:
        return base_path / (row.replace(" ", "_"))
    except Exception:
        logging.exception("Failed to convert and update path")
        raise


def populate_submissions() -> None:
    """
    populate_submissions: Populate the 'assignments_submissions' table with student submission records.

    For each assignment, this function:
      - Identifies corresponding bulk submission directories
      - Reads `.cg-info.json` metadata and associated CSV grade files
      - Merges data with student records to resolve foreign keys
      - Writes the processed submission records into the database

    Parameters
    ----------
    None

    Returns
    -------
    None

    Affected Table
    --------------
    - assignments_submissions
    """
    try:
        # Read tables in dataframes
        df_assignment = read_assignments()
        df_bulk = read_bulksubmissions()
        df_students = read_course_student()[["id", "codeGrade_id"]].rename(columns={"id": "student_id"})
        df_info = None
        df_csv = None

        for idx in df_assignment.index:
            assignment_id = df_assignment.loc[idx, "id"]
            due_date = df_assignment.loc[idx, "due_date"]
            bulk_submission_path = BASE_PATH_PRISM / f"assignments/assignment_{assignment_id} / bulk_submission"
            bulk_dirs = df_bulk[df_bulk["assignment_id"] == assignment_id]

            # Iterate over each course instance's submission directory
            for path in bulk_submission_path.iterdir():
                if not path.suffix:
                    # Resolve course_instance_id using directory_path match
                    course_instance_id = bulk_dirs.loc[bulk_dirs["directory_path"] == str(path), "course_instance_id"].iloc[0]

                    # Load CodeGrade JSON metadata
                    df_info = pd.read_json(path / ".cg-info.json").reset_index().rename(columns={"user_ids": "codeGrade_id", "index": "file_path"})
                    df_info["file_path"] = df_info["file_path"].apply(lambda row: convert_and_update(row, path))

                    # Load grades from accompanying CSV
                    df_csv = pd.read_csv(path.with_suffix(".csv"))[["Id", "Grade"]].rename(columns={"Id": "codeGrade_id", "Grade": "grade"})

                    # Merge all metadata: grades, student IDs, and course info
                    df_info = df_info.merge(df_csv, how="left", on="codeGrade_id")
                    df_info = df_info.merge(df_students, how="left", on="codeGrade_id")[["student_id", "file_path", "grade"]]

                    # Add additional required columns
                    df_info[["flagged", "assignment_id", "course_instance_id", "created_at"]] = pd.DataFrame({"flagged": [False] * len(df_info), "assignment_id": [assignment_id] * len(df_info), "course_instance_id": [course_instance_id] * len(df_info), "created_at": [due_date] * len(df_info)})
                    df_info["file_path"] = df_info["file_path"].astype(str)

                    # Append dataframe's contents to database table
                    df_info.to_sql("assignments_submissions", ENGINE, if_exists="append", index=False)
                    logging.info("Successfully appended dataframe to 'assignments_submissions' table.")

        logging.info("Successfully populated 'assignments_submissions' table.")

    except Exception:
        logging.exception("Failed to populate 'assignments_submissions' table.")
        raise


def read_submissions(print_df: bool = False) -> None:
    """
    read_submissions: Read and return the contents of the 'assignments_submissions' table as a Pandas DataFrame.

    Converts all columns to appropriate types and optionally prints the DataFrame
    and its data types for inspection.

    Parameters
    ----------
    print_df : bool, optional
        If True, prints the loaded DataFrame and its data types. Default is False.

    Returns
    -------
    pd.DataFrame
        A DataFrame containing student submission records.

        Columns:
            - id (int64): Primary key
            - assignment_id (int64): FK to 'assignments_assignments.id'
            - course_instance_id (int64): FK to 'courses_courseinstances.id'
            - student_id (int64): FK to 'courses_students.id'
            - file_path (str): Relative path to the submitted file
            - grade (float64): Grade earned by the student
            - flagged (bool): Whether the submission has been flagged
            - created_at (datetime): Date/time the submission was created
    """
    try:
        # Column names and types
        bigint_columns = ["id", "assignment_id", "course_instance_id", "student_id"]
        boolean_columns = ["flagged"]
        date_columns = ["created_at"]
        float_columns = ["grade"]
        string_columns = ["file_path"]

        # Load assignments_submissions table into a dataframe
        df_submission = pd.read_sql("SELECT * FROM assignments_submissions", ENGINE)

        # Alter column data types
        df_submission[boolean_columns] = df_submission[boolean_columns].astype(bool)
        df_submission[bigint_columns] = df_submission[bigint_columns].astype("int64")
        for col in date_columns:
            df_submission[col] = pd.to_datetime(df_submission[col])
        df_submission[float_columns] = df_submission[float_columns].astype("float64")
        df_submission[string_columns] = df_submission[string_columns].astype(str)

        # Print dataframes
        if print_df:
            print(f"df_submission:\n{df_submission}\n{df_submission.dtypes}")

        return df_submission

    except Exception:
        logging.exception("Failed to read 'assignments_submissions'")
        raise


def isolate_percentage(row: pd.Series) -> int:
    """
    isolate_percentage: Extracts and returns the greater of two percentage values embedded in the filenames of 'File 1' and 'File 2' columns.

    Assumes filenames follow a pattern like '/PRISM/data/assignments/assignment_1/bulk_submission/CS_135_1001_-_2024_Sprg_-_Assignment_0/6414_-_Jon_Powers/ (99%)', where 'XX'
    is the percentage to extract.

    Example:
        File 1: /PRISM/data/assignments/assignment_1/bulk_submission/CS_135_1001_-_2024_Sprg_-_Assignment_0/6355_-_Bertha_Love/ (19%)
        File 2: /PRISM/data/assignments/assignment_1/bulk_submission/CS_135_1001_-_2024_Sprg_-_Assignment_0/6414_-_Jon_Powers/ (16%)
        Result: 19

    Parameters
    ----------
    row : pd.Series
        A row containing 'File 1' and 'File 2' as Path-like objects.

    Returns
    -------
    int
        The higher extracted percentage between File 1 and File 2.
    """
    try:
        return max(int(row["File 1"].stem[2:][:-2]), int(row["File 2"].stem[2:][:-2]))
    except Exception:
        logging.exception("Failed to isolate percentage from input strings")
        raise


def correct_file_paths(row: pd.Series) -> pd.Series:
    """
    correct_file_paths: Modify a row by replacing 'File 1' and 'File 2' with their parent directories.

    Intended for use with `DataFrame.apply(..., axis=1)`.

    Parameters
    ----------
    row : pd.Series
        A row containing 'File 1' and 'File 2' as Path objects.

    Returns
    -------
    pd.Series
        The same row, with 'File 1' and 'File 2' updated to their parent paths.
    """
    try:
        row["File 1"] = row["File 1"].parent
        row["File 2"] = row["File 2"].parent
        return row
    except Exception:
        logging.exception("Failed to correct file paths")
        raise


def parse_moss_populate_similarities() -> None:
    """
    parse_moss_populate_similarities: Parse MOSS similarity reports and populate the cheating_submissionsimilaritypairs table.

    For each assignment, this function:
      - Reads the MOSS HTML report
      - Extracts file paths and percentages
      - Resolves file paths to student submission IDs
      - Filters duplicates to keep only the highest percentage per submission pair
      - Inserts similarity pairs into the database

    Parameters
    ----------
    None

    Returns
    -------
    None

    Affected Table
    --------------
    - cheating_submissionsimilaritypairs
    """
    try:
        df_assignments = read_assignments()
        df_submission = read_submissions()[["id", "assignment_id", "file_path"]].rename(columns={"id": "submission_id"})
        df_assignments["moss_report_directory_path"] = df_assignments["moss_report_directory_path"].apply(Path)

        for idx in df_assignments.index:
            assignment_id = df_assignments.loc[idx, "id"]

            # Load HTML table from MOSS report
            df_similarities = pd.read_html(df_assignments.loc[idx, "moss_report_directory_path"] / "index.html")[0].drop(columns=["Lines Matched"])

            # Convert path strings to Path objects
            df_similarities["File 1"] = df_similarities["File 1"].apply(Path)
            df_similarities["File 2"] = df_similarities["File 2"].apply(Path)

            # Extract percentage value from filenames
            df_similarities["percentage"] = df_similarities.apply(isolate_percentage, axis=1)

            # Normalize file paths (remove filename, keep directory only)
            # Reset index to serve as match_id
            df_similarities = df_similarities.apply(correct_file_paths, axis=1).reset_index().rename(columns={"index": "match_id"})

            # Add additional field
            df_similarities[["file_name", "assignment_id"]] = pd.DataFrame({"file_name": ["main.cpp"] * len(df_similarities), "assignment_id": [assignment_id] * len(df_similarities)})
            df_similarities[["File 1", "File 2"]] = df_similarities[["File 1", "File 2"]].astype(str)
            df_similarities["percentage"] = df_similarities["percentage"].astype("int32")

            # Match File 1 and File 2 to student submission IDs
            df_similarities = df_similarities.merge(df_submission, how="left", left_on=["assignment_id", "File 1"], right_on=["assignment_id", "file_path"]).drop(columns=["File 1", "file_path"]).rename(columns={"submission_id": "submission_id_1"})
            df_similarities = df_similarities.merge(df_submission, how="left", left_on=["assignment_id", "File 2"], right_on=["assignment_id", "file_path"]).drop(columns=["File 2", "file_path"]).rename(columns={"submission_id": "submission_id_2"})

            # Remove duplicate
            df_duplicates = df_similarities[df_similarities.duplicated(subset=["submission_id_1", "submission_id_2", "assignment_id"], keep=False)]
            df_keep = df_duplicates.groupby(["submission_id_1", "submission_id_2", "assignment_id"])["percentage"].idxmax()
            df_similarities.drop(df_duplicates[~df_duplicates.index.isin(df_keep)].index, axis=0, inplace=True)

            # Append dataframe's contents to database table
            df_similarities.to_sql("cheating_submissionsimilaritypairs", ENGINE, if_exists="append", index=False)
            logging.info("Successfully appended dataframe to 'cheating_submissionsimilaritypairs' table.")

        logging.info("Successfully populated 'cheating_submissionsimilaritypairs' table.")

    except Exception:
        logging.exception("Failed to populate 'cheating_submissionsimilaritypairs' table.")
        raise


if __name__ == "__main__":
    main()
